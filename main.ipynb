{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sz4KmoLdM3lT",
        "outputId": "f713ec8c-9c82-452a-bd83-32299e6f2d44"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib\n",
        "import torch\n",
        "import datetime\n",
        "import csv\n",
        "\n",
        "import gymnasium as gym\n",
        "import gymnasium.wrappers as gym_wrap\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import DQN_model as DQN\n",
        "\n",
        "from gymnasium.spaces import Box\n",
        "from tensordict import TensorDict\n",
        "from torch import nn\n",
        "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage\n",
        "\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train a model from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "xH8_xOOIfE6G",
        "outputId": "a0bf779a-5e83-4deb-9500-b4fe24dd2bf8"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CarRacing-v2\", continuous=False)\n",
        "env = DQN.SkipFrame(env, skip=4)\n",
        "env = gym_wrap.GrayScaleObservation(env)\n",
        "env = gym_wrap.ResizeObservation(env, shape=84)\n",
        "env = gym_wrap.FrameStack(env, num_stack=4)\n",
        "state, info = env.reset()\n",
        "action_n = env.action_space.n\n",
        "driver = DQN.Agent(state.shape, action_n, double_q=True)\n",
        "batch_n = 32\n",
        "play_n_episodes = 3000\n",
        "episode_epsilon_list = []\n",
        "episode_reward_list = []\n",
        "episode_length_list = []\n",
        "episode_loss_list = []\n",
        "episode_date_list = []\n",
        "episode_time_list = []\n",
        "episode = 0\n",
        "timestep_n = 0\n",
        "when2learn = 4 # in timesteps\n",
        "when2sync = 5000 # in timesteps\n",
        "when2save = 100000 # in timesteps\n",
        "when2report = 5000 # in timesteps\n",
        "when2eval = 50000 # in timesteps\n",
        "when2log = 10 # in episodes\n",
        "report_type = 'plot' # 'text', 'plot', None\n",
        "\n",
        "while episode <= play_n_episodes:\n",
        "    \n",
        "    episode += 1\n",
        "    episode_reward = 0\n",
        "    episode_length = 0\n",
        "    updating = True\n",
        "    loss_list = []\n",
        "    episode_epsilon_list.append(driver.epsilon)\n",
        "\n",
        "    while updating:\n",
        "\n",
        "        timestep_n += 1\n",
        "        episode_length += 1\n",
        "        \n",
        "        action = driver.take_action(state)\n",
        "        new_state, reward, terminated, truncated, info = env.step(action)\n",
        "        episode_reward += reward\n",
        "        driver.store(state, action, reward, new_state, terminated)\n",
        "        # Move to the next state\n",
        "        state = new_state\n",
        "        updating = not (terminated or truncated)\n",
        "\n",
        "        if timestep_n % when2sync == 0:\n",
        "            upd_net_param = driver.updating_net.state_dict()\n",
        "            driver.frozen_net.load_state_dict(upd_net_param)\n",
        "            \n",
        "        if timestep_n % when2save == 0:\n",
        "            driver.save(driver.save_dir, 'DQN')\n",
        "            \n",
        "        if timestep_n % when2learn == 0:\n",
        "            q, loss = driver.update_net(batch_n)\n",
        "            loss_list.append(loss)\n",
        "        \n",
        "\n",
        "        if timestep_n % when2report == 0 and report_type == 'text':\n",
        "            print(f'Report: {timestep_n} timestep')\n",
        "            print(f'    episodes: {episode}')\n",
        "            print(f'    n_updates: {driver.n_updates}')\n",
        "            print(f'    epsilon: {driver.epsilon}')\n",
        "\n",
        "        if timestep_n % when2eval == 0 and report_type == 'text':\n",
        "            rewards_tensor = torch.tensor(episode_reward_list,\n",
        "                                          dtype=torch.float)\n",
        "            eval_reward = torch.clone(rewards_tensor[-50:])\n",
        "            mean_eval_reward = round(torch.mean(eval_reward).item(), 2)\n",
        "            std_eval_reward = round(torch.std(eval_reward).item(), 2)\n",
        "\n",
        "            lengths_tensor = torch.tensor(episode_length_list,\n",
        "                                          dtype=torch.float)\n",
        "            eval_length = torch.clone(lengths_tensor[-50:])\n",
        "            mean_eval_length = round(torch.mean(eval_length).item(), 2)\n",
        "            std_eval_length = round(torch.std(eval_length).item(), 2)\n",
        "\n",
        "            \n",
        "            print(f'Evaluation: {timestep_n} timestep')\n",
        "            print(f'    reward {mean_eval_reward}±{std_eval_reward}')\n",
        "            print(f'    episode length {mean_eval_length}±{std_eval_length}')\n",
        "            print(f'    episodes: {episode}')\n",
        "            print(f'    n_updates: {driver.n_updates}')\n",
        "            print(f'    epsilon: {driver.epsilon}')\n",
        "\n",
        "    state, info = env.reset()\n",
        "    \n",
        "    episode_reward_list.append(episode_reward)\n",
        "    episode_length_list.append(episode_length)\n",
        "    episode_loss_list.append(np.mean(loss_list))\n",
        "    now_time = datetime.datetime.now()\n",
        "    episode_date_list.append(now_time.date().strftime('%Y-%m-%d'))\n",
        "    episode_time_list.append(now_time.time().strftime('%H:%M:%S'))\n",
        "    \n",
        "    if report_type == 'plot':\n",
        "        draw_check = DQN.plot_reward(episode, episode_reward_list, timestep_n)\n",
        "\n",
        "    if episode % when2log == 0:\n",
        "        driver.write_log(\n",
        "            episode_date_list,\n",
        "            episode_time_list,\n",
        "            episode_reward_list,\n",
        "            episode_length_list,\n",
        "            episode_loss_list,\n",
        "            episode_epsilon_list,\n",
        "            log_filename='DQN_log_test.csv'\n",
        "            )\n",
        "\n",
        "if report_type == 'text':\n",
        "    rewards_tensor = torch.tensor(episode_reward_list, dtype=torch.float)\n",
        "    eval_reward = torch.clone(rewards_tensor[-100:])\n",
        "    mean_eval_reward = round(torch.mean(eval_reward).item(), 2)\n",
        "    std_eval_reward = round(torch.std(eval_reward).item(), 2)\n",
        "\n",
        "    lengths_tensor = torch.tensor(episode_length_list, dtype=torch.float)\n",
        "    eval_length = torch.clone(lengths_tensor[-100:])\n",
        "    mean_eval_length = round(torch.mean(eval_length).item(), 2)\n",
        "    std_eval_length = round(torch.std(eval_length).item(), 2)\n",
        "    \n",
        "    print(f'Final evaluation: {timestep_n} timestep')\n",
        "    print(f'    reward {mean_eval_reward}±{std_eval_reward}')\n",
        "    print(f'    episode length {mean_eval_length}±{std_eval_length}')\n",
        "    print(f'    episodes: {episode}')\n",
        "    print(f'    n_updates: {driver.n_updates}')\n",
        "    print(f'    epsilon: {driver.epsilon}')\n",
        "    \n",
        "driver.save(driver.save_dir, 'DQN')\n",
        "driver.write_log(\n",
        "    episode_date_list,\n",
        "    episode_time_list,\n",
        "    episode_reward_list,\n",
        "    episode_length_list,\n",
        "    episode_loss_list,\n",
        "    episode_epsilon_list,\n",
        "    log_filename='DQN_log_test.csv'\n",
        "    )\n",
        "env.close()\n",
        "plt.ioff()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Continue training a loaded model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make(\"CarRacing-v2\", continuous=False)\n",
        "env = DQN.SkipFrame(env, skip=4)\n",
        "env = gym_wrap.GrayScaleObservation(env)\n",
        "env = gym_wrap.ResizeObservation(env, shape=84)\n",
        "env = gym_wrap.FrameStack(env, num_stack=4)\n",
        "state, info = env.reset()\n",
        "action_n = env.action_space.n\n",
        "driver = DQN.Agent(state.shape, action_n,\n",
        "               load_state='train', load_model='DDQN_741500.pt')\n",
        "batch_n = 32\n",
        "actions_taken = driver.act_taken\n",
        "play_more_episodes = 5\n",
        "episodes_played = 3000\n",
        "timestep_n = 740188\n",
        "episode = episodes_played\n",
        "episode_epsilon_list = []\n",
        "episode_reward_list = []\n",
        "episode_length_list = []\n",
        "episode_loss_list = []\n",
        "episode_date_list = []\n",
        "episode_time_list = []\n",
        "when2learn = 4 # in timesteps\n",
        "when2sync = 5000 # in timesteps\n",
        "when2save = 100000 # in timesteps\n",
        "when2report = 5000 # in timesteps\n",
        "when2eval = 50000 # in timesteps\n",
        "when2log = 10 # in episodes\n",
        "report_type = 'plot' # 'text', 'plot', None\n",
        "print(f'Initial exploration: {driver.epsilon}, Actions already taken: {actions_taken}, Episodes already played: {episode}')\n",
        "while episode <= episodes_played + play_more_episodes:\n",
        "    \n",
        "    episode += 1\n",
        "    episode_reward = 0\n",
        "    episode_length = 0\n",
        "    updating = True\n",
        "    loss_list = []\n",
        "    episode_epsilon_list.append(driver.epsilon)\n",
        "\n",
        "    while updating:\n",
        "\n",
        "        timestep_n += 1\n",
        "        episode_length += 1\n",
        "        \n",
        "        action = driver.take_action(state)\n",
        "        new_state, reward, terminated, truncated, info = env.step(action)\n",
        "        episode_reward += reward\n",
        "        driver.store(state, action, reward, new_state, terminated)\n",
        "        # Move to the next state\n",
        "        state = new_state\n",
        "        updating = not (terminated or truncated)\n",
        "\n",
        "        if timestep_n % when2sync == 0:\n",
        "            upd_net_param = driver.updating_net.state_dict()\n",
        "            driver.frozen_net.load_state_dict(upd_net_param)\n",
        "            \n",
        "        if timestep_n % when2save == 0:\n",
        "            driver.save(driver.save_dir, 'DQN')\n",
        "            \n",
        "        if timestep_n % when2learn == 0:\n",
        "            q, loss = driver.update_net(batch_n)\n",
        "            loss_list.append(loss)\n",
        "        \n",
        "\n",
        "        if timestep_n % when2report == 0 and report_type == 'text':\n",
        "            print(f'Report: {timestep_n} timestep')\n",
        "            print(f'    episodes: {episode}')\n",
        "            print(f'    n_updates: {driver.n_updates}')\n",
        "            print(f'    epsilon: {driver.epsilon}')\n",
        "\n",
        "        if timestep_n % when2eval == 0 and report_type == 'text':\n",
        "            rewards_tensor = torch.tensor(episode_reward_list,\n",
        "                                          dtype=torch.float)\n",
        "            eval_reward = torch.clone(rewards_tensor[-50:])\n",
        "            mean_eval_reward = round(torch.mean(eval_reward).item(), 2)\n",
        "            std_eval_reward = round(torch.std(eval_reward).item(), 2)\n",
        "\n",
        "            lengths_tensor = torch.tensor(episode_length_list,\n",
        "                                          dtype=torch.float)\n",
        "            eval_length = torch.clone(lengths_tensor[-50:])\n",
        "            mean_eval_length = round(torch.mean(eval_length).item(), 2)\n",
        "            std_eval_length = round(torch.std(eval_length).item(), 2)\n",
        "\n",
        "            \n",
        "            print(f'Evaluation: {timestep_n} timestep')\n",
        "            print(f'    reward {mean_eval_reward}±{std_eval_reward}')\n",
        "            print(f'    episode length {mean_eval_length}±{std_eval_length}')\n",
        "            print(f'    episodes: {episode}')\n",
        "            print(f'    n_updates: {driver.n_updates}')\n",
        "            print(f'    epsilon: {driver.epsilon}')\n",
        "\n",
        "    state, info = env.reset()\n",
        "    \n",
        "    episode_reward_list.append(episode_reward)\n",
        "    episode_length_list.append(episode_length)\n",
        "    episode_loss_list.append(np.mean(loss_list))\n",
        "    now_time = datetime.datetime.now()\n",
        "    episode_date_list.append(now_time.date().strftime('%Y-%m-%d'))\n",
        "    episode_time_list.append(now_time.time().strftime('%H:%M:%S'))\n",
        "    \n",
        "    if report_type == 'plot':\n",
        "        draw_check = DQN.plot_reward(episode, episode_reward_list, timestep_n)\n",
        "\n",
        "    if episode % when2log == 0:\n",
        "        driver.write_log(\n",
        "            episode_date_list,\n",
        "            episode_time_list,\n",
        "            episode_reward_list,\n",
        "            episode_length_list,\n",
        "            episode_loss_list,\n",
        "            episode_epsilon_list,\n",
        "            log_filename='DQN_log_test.csv'\n",
        "            )\n",
        "\n",
        "if report_type == 'text':\n",
        "    rewards_tensor = torch.tensor(episode_reward_list, dtype=torch.float)\n",
        "    eval_reward = torch.clone(rewards_tensor[-100:])\n",
        "    mean_eval_reward = round(torch.mean(eval_reward).item(), 2)\n",
        "    std_eval_reward = round(torch.std(eval_reward).item(), 2)\n",
        "\n",
        "    lengths_tensor = torch.tensor(episode_length_list, dtype=torch.float)\n",
        "    eval_length = torch.clone(lengths_tensor[-100:])\n",
        "    mean_eval_length = round(torch.mean(eval_length).item(), 2)\n",
        "    std_eval_length = round(torch.std(eval_length).item(), 2)\n",
        "    \n",
        "    print(f'Final evaluation: {timestep_n} timestep')\n",
        "    print(f'    reward {mean_eval_reward}±{std_eval_reward}')\n",
        "    print(f'    episode length {mean_eval_length}±{std_eval_length}')\n",
        "    print(f'    episodes: {episode}')\n",
        "    print(f'    n_updates: {driver.n_updates}')\n",
        "    print(f'    epsilon: {driver.epsilon}')\n",
        "\n",
        "driver.save(driver.save_dir, 'DQN')\n",
        "driver.write_log(\n",
        "    episode_date_list,\n",
        "    episode_time_list,\n",
        "    episode_reward_list,\n",
        "    episode_length_list,\n",
        "    episode_loss_list,\n",
        "    episode_epsilon_list,\n",
        "    log_filename='DQN_log_test.csv'\n",
        "    )\n",
        "env.close()\n",
        "plt.ioff()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loaded model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make(\"CarRacing-v2\", continuous=False, render_mode=\"human\")\n",
        "env = DQN.SkipFrame(env, skip=4)\n",
        "env = gym_wrap.GrayScaleObservation(env)\n",
        "env = gym_wrap.ResizeObservation(env, shape=84)\n",
        "env = gym_wrap.FrameStack(env, num_stack=4)\n",
        "state, info = env.reset()\n",
        "action_n = env.action_space.n\n",
        "driver = DQN.Agent(state.shape, action_n,\n",
        "               load_state='eval', load_model='DQN_740188.pt')\n",
        "driver.epsilon = 0\n",
        "episodes = 1\n",
        "scores_array = []\n",
        "timestep_arr = []\n",
        "seeds_list = [i for i in range(2)] # List of any seeds can be specified\n",
        "\n",
        "for episode, sd in enumerate(seeds_list):\n",
        "    state, info = env.reset(seed=sd)\n",
        "    updating = True\n",
        "    score = 0\n",
        "    timestep = 0\n",
        "    \n",
        "    while updating:\n",
        "        action = driver.take_action(state) \n",
        "        state, reward, terminated, truncated, info = env.step(action)\n",
        "        updating = not (terminated or truncated)\n",
        "        score += reward\n",
        "        timestep += 1\n",
        "    scores_array.append(score)\n",
        "    timestep_arr.append(timestep)\n",
        "    print(f\"Episode:{episode}, Score:{score:.2f}, Timesteps: {timestep}\")\n",
        "\n",
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

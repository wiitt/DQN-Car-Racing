{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sz4KmoLdM3lT",
        "outputId": "f713ec8c-9c82-452a-bd83-32299e6f2d44"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib\n",
        "import torch\n",
        "import datetime\n",
        "import csv\n",
        "\n",
        "import gymnasium as gym\n",
        "import gymnasium.wrappers as gym_wrap\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from gymnasium.spaces import Box\n",
        "from tensordict import TensorDict\n",
        "from torch import nn\n",
        "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage\n",
        "\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8JLRYzlM6SR"
      },
      "outputs": [],
      "source": [
        "class SkipFrame(gym.Wrapper):\n",
        "    def __init__(self, env, skip):\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0.0\n",
        "        for _ in range(self._skip):\n",
        "            state, reward, terminated, truncated, info = self.env.step(action)\n",
        "            total_reward += reward\n",
        "            if terminated:\n",
        "                break\n",
        "        return state, total_reward, terminated, truncated, info\n",
        "\n",
        "\n",
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super().__init__()\n",
        "        cannel_n, height, width = in_dim\n",
        "\n",
        "        if height != 84 or width != 84:\n",
        "            raise ValueError(f\"DQN model requires input of a (84, 84)-shape. Input of a ({height, width})-shape was passed.\")\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=cannel_n, out_channels=16,\n",
        "                      kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=16, out_channels=32,\n",
        "                      kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(2592, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, out_dim),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.net(input)\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(\n",
        "            self,\n",
        "            state_space_shape,\n",
        "            action_n,\n",
        "            load_state=False,\n",
        "            load_model=None,\n",
        "            double_q=False,\n",
        "            gamma = 0.95,\n",
        "            epsilon = 1,\n",
        "            epsilon_decay = 0.9999925,\n",
        "            epsilon_min = 0.05\n",
        "            ):\n",
        "        \n",
        "        self.gamma = gamma # discounting factor\n",
        "        self.epsilon = epsilon # exploration rate\n",
        "        self.epsilon_decay = epsilon_decay # decay of exploration rate\n",
        "        self.epsilon_min = epsilon_min # minimum value of exploration rate\n",
        "        self.state_shape = state_space_shape\n",
        "        self.action_n = action_n\n",
        "        self.load_state = load_state # purpose of loading the model (training or evaluation)\n",
        "        self.double_q = double_q # if True, Double DQN is used\n",
        "        self.save_dir = './Training/Saved_Models/MyDDQN/'\n",
        "        self.log_dir = './Training/Logs/DDQN/'\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.updating_net = DQN(self.state_shape, self.action_n).float()\n",
        "        self.updating_net = self.updating_net.to(device=self.device)\n",
        "        self.frozen_net = DQN(self.state_shape, self.action_n).float()\n",
        "        self.frozen_net = self.frozen_net.to(device=self.device)\n",
        "        self.optimizer = torch.optim.Adam(self.updating_net.parameters(),\n",
        "                                          lr=0.0002)\n",
        "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
        "        self.buffer = TensorDictReplayBuffer(\n",
        "                storage=LazyMemmapStorage(\n",
        "                    300000,\n",
        "                    device=torch.device(\"cpu\")))\n",
        "        self.act_taken = 0\n",
        "        self.n_updates = 0\n",
        "        if load_state:\n",
        "            if load_model == None:\n",
        "                raise ValueError(f\"Specify a model name for loading.\")\n",
        "            load_dir = self.save_dir\n",
        "            self.load_model = load_model\n",
        "            self.load(load_dir, load_model)\n",
        "        \n",
        "\n",
        "    def store(self, state, action, reward, new_state, terminated):\n",
        "        self.buffer.add(TensorDict({\n",
        "                    \"state\": torch.tensor(state),\n",
        "                    \"action\": torch.tensor(action),\n",
        "                    \"reward\": torch.tensor(reward),\n",
        "                    \"new_state\": torch.tensor(new_state),\n",
        "                    \"terminated\": torch.tensor(terminated)\n",
        "                    }, batch_size=[]))\n",
        "\n",
        "    def get_samples(self, batch_size):\n",
        "        batch = self.buffer.sample(\n",
        "            batch_size)\n",
        "        states = batch.get('state').type(torch.FloatTensor).to(self.device)\n",
        "        new_states = batch.get('new_state').type(torch.FloatTensor).to(self.device)\n",
        "        actions = batch.get('action').squeeze().to(self.device)\n",
        "        rewards = batch.get('reward').squeeze().to(self.device)\n",
        "        terminateds = batch.get('terminated').squeeze().to(self.device)\n",
        "        return states, actions, rewards, new_states, terminateds\n",
        "\n",
        "    def take_action(self, state):\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            action_idx = np.random.randint(self.action_n)\n",
        "        else:\n",
        "            state = torch.tensor(\n",
        "                state,\n",
        "                dtype=torch.float32,\n",
        "                device=self.device\n",
        "                ).unsqueeze(0)\n",
        "            action_values = self.updating_net(state)\n",
        "            action_idx = torch.argmax(action_values, axis=1).item()\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "        else:\n",
        "            self.epsilon = self.epsilon_min\n",
        "        self.act_taken += 1\n",
        "        return action_idx\n",
        "\n",
        "    def update_net(self, batch_size):\n",
        "        self.n_updates += 1\n",
        "        states, actions, rewards, \\\n",
        "            new_states, terminateds = self.get_samples(batch_size)\n",
        "        action_values = self.updating_net(states)\n",
        "        td_est = action_values[np.arange(batch_size), actions]\n",
        "        if self.double_q:\n",
        "            with torch.no_grad():\n",
        "                next_actions = torch.argmax(self.updating_net(new_states), axis=1)\n",
        "                tar_action_values = self.frozen_net(new_states)\n",
        "            td_tar = rewards + (1 - terminateds.float()) \\\n",
        "                * self.gamma*tar_action_values[np.arange(batch_size), next_actions]\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                tar_action_values = self.frozen_net(new_states)\n",
        "            td_tar = rewards + (1 - terminateds.float()) * self.gamma*tar_action_values.max(1)[0]\n",
        "        loss = self.loss_fn(td_est, td_tar)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        loss = loss.item()\n",
        "        return td_est, loss\n",
        "\n",
        "    def save(self, save_dir, save_name):\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.makedirs(save_dir)\n",
        "        save_path = save_dir + save_name + f\"_{self.act_taken}.pt\"\n",
        "        torch.save({\n",
        "            'upd_model_state_dict': self.updating_net.state_dict(),\n",
        "            'frz_model_state_dict': self.frozen_net.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'replay_buffer': self.buffer,\n",
        "            'action_number': self.act_taken,\n",
        "            'epsilon': self.epsilon\n",
        "            }, save_path)\n",
        "        print(f\"Model saved to {save_path} at step {self.act_taken}\")\n",
        "\n",
        "    def load(self, load_dir, model_name):\n",
        "        loaded_model = torch.load(load_dir+model_name)\n",
        "        upd_net_param = loaded_model['upd_model_state_dict']\n",
        "        frz_net_param = loaded_model['frz_model_state_dict']\n",
        "        opt_param = loaded_model['optimizer_state_dict']\n",
        "        self.updating_net.load_state_dict(upd_net_param)\n",
        "        self.frozen_net.load_state_dict(frz_net_param)\n",
        "        self.optimizer.load_state_dict(opt_param)\n",
        "        if self.load_state == 'eval':\n",
        "            self.updating_net.eval()\n",
        "            self.frozen_net.eval()\n",
        "            self.epsilon_min = 0\n",
        "            self.epsilon = 0\n",
        "        elif self.load_state == 'train':\n",
        "            self.updating_net.train()\n",
        "            self.frozen_net.train()\n",
        "            # self.buffer = loaded_model['replay_buffer']\n",
        "            self.act_taken = loaded_model['action_number']\n",
        "            self.epsilon = 0.05\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown load state. Should be either 'eval' or 'train'.\")\n",
        "        \n",
        "    def write_log(\n",
        "            self,\n",
        "            date_list,\n",
        "            time_list,\n",
        "            reward_list,\n",
        "            length_list,\n",
        "            loss_list,\n",
        "            epsilon_list,\n",
        "            log_filename='default_log.csv'\n",
        "            ):\n",
        "\n",
        "        if not os.path.exists(self.log_dir):\n",
        "            os.makedirs(self.log_dir)\n",
        "        rows = [['date']+date_list,\n",
        "                ['time']+time_list,\n",
        "                ['reward']+reward_list,\n",
        "                ['length']+length_list,\n",
        "                ['loss']+loss_list,\n",
        "                ['epsilon']+epsilon_list]\n",
        "        with open(self.log_dir+log_filename, 'w') as csvfile:  \n",
        "            csvwriter = csv.writer(csvfile)    \n",
        "            csvwriter.writerows(rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYs5j0D0fBIP"
      },
      "outputs": [],
      "source": [
        "def plot_reward(episode_num, reward_list, n_steps):\n",
        "    plt.figure(1)\n",
        "    rewards_tensor = torch.tensor(reward_list, dtype=torch.float)\n",
        "    if len(rewards_tensor) >= 11:\n",
        "        eval_reward = torch.clone(rewards_tensor[-10:])\n",
        "        mean_eval_reward = round(torch.mean(eval_reward).item(), 2)\n",
        "        std_eval_reward = round(torch.std(eval_reward).item(), 2)\n",
        "        plt.clf()\n",
        "        plt.title(f'Episode #{episode_num}: {n_steps} steps, \\\n",
        "                  reward {mean_eval_reward}±{std_eval_reward}')\n",
        "    else:\n",
        "        plt.clf()\n",
        "        plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Reward')\n",
        "    plt.plot(rewards_tensor.numpy())\n",
        "    if len(rewards_tensor) >= 50:\n",
        "        reward_f = torch.clone(rewards_tensor[:50])\n",
        "        means = rewards_tensor.unfold(0, 50, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.ones(49)*torch.mean(reward_f), means))\n",
        "        plt.plot(means.numpy())\n",
        "    plt.pause(0.001)\n",
        "    if is_ipython:\n",
        "        display.display(plt.gcf())\n",
        "        display.clear_output(wait=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train a model from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "xH8_xOOIfE6G",
        "outputId": "a0bf779a-5e83-4deb-9500-b4fe24dd2bf8"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CarRacing-v2\", continuous=False)\n",
        "env = SkipFrame(env, skip=4)\n",
        "env = gym_wrap.GrayScaleObservation(env)\n",
        "env = gym_wrap.ResizeObservation(env, shape=84)\n",
        "env = gym_wrap.FrameStack(env, num_stack=4)\n",
        "state, info = env.reset()\n",
        "updating = True\n",
        "action_n = env.action_space.n\n",
        "driver = Agent(state.shape, action_n, double_q=True)\n",
        "batch_n = 32\n",
        "# take_n_action = 400000\n",
        "play_n_episodes = 2500\n",
        "episode_epsilon_list = []\n",
        "episode_reward_list = []\n",
        "episode_length_list = []\n",
        "episode_loss_list = []\n",
        "episode_date_list = []\n",
        "episode_time_list = []\n",
        "episode = 0\n",
        "timestep_n = 0\n",
        "when2learn = 4 # in timesteps\n",
        "when2sync = 5000 # in timesteps\n",
        "when2save = 100000 # in timesteps\n",
        "when2report = 5000 # in timesteps\n",
        "when2eval = 50000 # in timesteps\n",
        "when2log = 10 # in episodes\n",
        "report_eval = False\n",
        "\n",
        "while episode <= play_n_episodes:\n",
        "    \n",
        "    episode += 1\n",
        "    episode_reward = 0\n",
        "    episode_length = 0\n",
        "    updating = True\n",
        "    loss_list = []\n",
        "    episode_epsilon_list.append(driver.epsilon)\n",
        "\n",
        "    while updating:\n",
        "\n",
        "        timestep_n += 1\n",
        "        episode_length += 1\n",
        "        \n",
        "        action = driver.take_action(state)\n",
        "        new_state, reward, terminated, truncated, info = env.step(action)\n",
        "        episode_reward += reward\n",
        "        driver.store(state, action, reward, new_state, terminated)\n",
        "        # Move to the next state\n",
        "        state = new_state\n",
        "        updating = not (terminated or truncated)\n",
        "\n",
        "        if timestep_n % when2sync == 0:\n",
        "            upd_net_param = driver.updating_net.state_dict()\n",
        "            driver.frozen_net.load_state_dict(upd_net_param)\n",
        "            \n",
        "        if timestep_n % when2save == 0:\n",
        "            driver.save(driver.save_dir, 'DDQN')\n",
        "            \n",
        "        if timestep_n % when2learn == 0:\n",
        "            q, loss = driver.update_net(batch_n)\n",
        "            loss_list.append(loss)\n",
        "        \n",
        "\n",
        "        if timestep_n % when2report == 0 and report_eval:\n",
        "            print(f'Report: {timestep_n} timestep')\n",
        "            print(f'    episodes: {episode}')\n",
        "            print(f'    n_updates: {driver.n_updates}')\n",
        "            print(f'    epsilon: {driver.epsilon}')\n",
        "\n",
        "        if timestep_n % when2eval == 0 and report_eval:\n",
        "            rewards_tensor = torch.tensor(episode_reward_list,\n",
        "                                          dtype=torch.float)\n",
        "            eval_reward = torch.clone(rewards_tensor[-50:])\n",
        "            mean_eval_reward = round(torch.mean(eval_reward).item(), 2)\n",
        "            std_eval_reward = round(torch.std(eval_reward).item(), 2)\n",
        "\n",
        "            lengths_tensor = torch.tensor(episode_length_list,\n",
        "                                          dtype=torch.float)\n",
        "            eval_length = torch.clone(lengths_tensor[-50:])\n",
        "            mean_eval_length = round(torch.mean(eval_length).item(), 2)\n",
        "            std_eval_length = round(torch.std(eval_length).item(), 2)\n",
        "\n",
        "            \n",
        "            print(f'Evaluation: {timestep_n} timestep')\n",
        "            print(f'    reward {mean_eval_reward}±{std_eval_reward}')\n",
        "            print(f'    episode length {mean_eval_length}±{std_eval_length}')\n",
        "            print(f'    episodes: {episode}')\n",
        "            print(f'    n_updates: {driver.n_updates}')\n",
        "            print(f'    epsilon: {driver.epsilon}')\n",
        "    \n",
        "    if report_eval:\n",
        "        rewards_tensor = torch.tensor(episode_reward_list, dtype=torch.float)\n",
        "        eval_reward = torch.clone(rewards_tensor[-100:])\n",
        "        mean_eval_reward = round(torch.mean(eval_reward).item(), 2)\n",
        "        std_eval_reward = round(torch.std(eval_reward).item(), 2)\n",
        "\n",
        "        lengths_tensor = torch.tensor(episode_length_list, dtype=torch.float)\n",
        "        eval_length = torch.clone(lengths_tensor[-100:])\n",
        "        mean_eval_length = round(torch.mean(eval_length).item(), 2)\n",
        "        std_eval_length = round(torch.std(eval_length).item(), 2)\n",
        "        \n",
        "        print(f'Final evaluation: {timestep_n} timestep')\n",
        "        print(f'    reward {mean_eval_reward}±{std_eval_reward}')\n",
        "        print(f'    episode length {mean_eval_length}±{std_eval_length}')\n",
        "        print(f'    episodes: {episode}')\n",
        "        print(f'    n_updates: {driver.n_updates}')\n",
        "        print(f'    epsilon: {driver.epsilon}')\n",
        "\n",
        "    state, info = env.reset()\n",
        "    \n",
        "    episode_reward_list.append(episode_reward)\n",
        "    episode_length_list.append(episode_length)\n",
        "    episode_loss_list.append(np.mean(loss_list))\n",
        "    now_time = datetime.datetime.now()\n",
        "    episode_date_list.append(now_time.date().strftime('%Y-%m-%d'))\n",
        "    episode_time_list.append(now_time.time().strftime('%H:%M:%S'))\n",
        "    draw_check = plot_reward(episode, episode_reward_list, timestep_n)\n",
        "\n",
        "    if episode % when2log == 0:\n",
        "        driver.write_log(\n",
        "            episode_date_list,\n",
        "            episode_time_list,\n",
        "            episode_reward_list,\n",
        "            episode_length_list,\n",
        "            episode_loss_list,\n",
        "            episode_epsilon_list,\n",
        "            log_filename='DDQN_log10.csv'\n",
        "        )\n",
        "\n",
        "plt.ioff()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Continue training a loaded model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make(\"CarRacing-v2\", continuous=False)\n",
        "env = SkipFrame(env, skip=4)\n",
        "env = gym_wrap.GrayScaleObservation(env)\n",
        "env = gym_wrap.ResizeObservation(env, shape=84)\n",
        "env = gym_wrap.FrameStack(env, num_stack=4)\n",
        "state, info = env.reset()\n",
        "action_n = env.action_space.n\n",
        "# driver = Agent(state.shape, action_n,\n",
        "            #    load_state='train', load_model='DQN_861590.pt')\n",
        "# driver.epsilon = 0.05\n",
        "# driver.epsilon_decay = 0.99999\n",
        "batch_n = 32\n",
        "actions_taken = driver.act_taken\n",
        "play_more_episodes = 500\n",
        "episodes_played = 4000\n",
        "# timestep_n = 861590\n",
        "# episode = episodes_played\n",
        "episode_epsilon_list = []\n",
        "episode_reward_list = []\n",
        "episode_length_list = []\n",
        "episode_loss_list = []\n",
        "episode_date_list = []\n",
        "episode_time_list = []\n",
        "when2report = 5000 # in timesteps\n",
        "when2eval = 50000 # in timesteps\n",
        "when2log = 10 # in episodes\n",
        "print(f'Initial exploration: {driver.epsilon}, Actions already taken: {actions_taken}, Episodes already played: {episode}')\n",
        "while episode <= episodes_played + play_more_episodes:\n",
        "    \n",
        "    episode += 1\n",
        "    episode_reward = 0\n",
        "    episode_length = 0\n",
        "    updating = True\n",
        "    loss_list = []\n",
        "    episode_epsilon_list.append(driver.epsilon)\n",
        "\n",
        "    while updating:\n",
        "\n",
        "        timestep_n += 1\n",
        "        episode_length += 1\n",
        "        \n",
        "        action = driver.take_action(state)\n",
        "        new_state, reward, terminated, truncated, info = env.step(action)\n",
        "        episode_reward += reward\n",
        "        driver.store(state, action, reward, new_state, terminated)\n",
        "        # Move to the next state\n",
        "        state = new_state\n",
        "        updating = not (terminated or truncated)\n",
        "\n",
        "        if timestep_n % driver.when2sync == 0:\n",
        "            upd_net_param = driver.updating_net.state_dict()\n",
        "            driver.frozen_net.load_state_dict(upd_net_param)\n",
        "            \n",
        "        if timestep_n % driver.when2save == 0:\n",
        "            driver.save(driver.save_dir, 'DQN')\n",
        "            \n",
        "        if timestep_n % driver.when2learn == 0:\n",
        "            q, loss = driver.update_net(batch_n)\n",
        "            loss_list.append(loss)\n",
        "        \n",
        "\n",
        "        # if timestep_n % when2report == 0:\n",
        "        #     print(f'Report: {timestep_n} timestep')\n",
        "        #     print(f'    episodes: {episode}')\n",
        "        #     print(f'    n_updates: {driver.n_updates}')\n",
        "        #     print(f'    epsilon: {driver.epsilon}')\n",
        "\n",
        "        # if timestep_n % when2eval == 0:\n",
        "        #     rewards_tensor = torch.tensor(episode_reward_list, dtype=torch.float)\n",
        "        #     eval_reward = torch.clone(rewards_tensor[-50:])\n",
        "        #     mean_eval_reward = round(torch.mean(eval_reward).item(), 2)\n",
        "        #     std_eval_reward = round(torch.std(eval_reward).item(), 2)\n",
        "\n",
        "        #     lengths_tensor = torch.tensor(episode_length_list, dtype=torch.float)\n",
        "        #     eval_length = torch.clone(lengths_tensor[-50:])\n",
        "        #     mean_eval_length = round(torch.mean(eval_length).item(), 2)\n",
        "        #     std_eval_length = round(torch.std(eval_length).item(), 2)\n",
        "\n",
        "            \n",
        "        #     print(f'Evaluation: {timestep_n} timestep')\n",
        "        #     print(f'    reward {mean_eval_reward}±{std_eval_reward}')\n",
        "        #     print(f'    episode length {mean_eval_length}±{std_eval_length}')\n",
        "        #     print(f'    episodes: {episode}')\n",
        "        #     print(f'    n_updates: {driver.n_updates}')\n",
        "        #     print(f'    epsilon: {driver.epsilon}')\n",
        "        \n",
        "    # rewards_tensor = torch.tensor(episode_reward_list, dtype=torch.float)\n",
        "    # eval_reward = torch.clone(rewards_tensor[-100:])\n",
        "    # mean_eval_reward = round(torch.mean(eval_reward).item(), 2)\n",
        "    # std_eval_reward = round(torch.std(eval_reward).item(), 2)\n",
        "\n",
        "    # lengths_tensor = torch.tensor(episode_length_list, dtype=torch.float)\n",
        "    # eval_length = torch.clone(lengths_tensor[-100:])\n",
        "    # mean_eval_length = round(torch.mean(eval_length).item(), 2)\n",
        "    # std_eval_length = round(torch.std(eval_length).item(), 2)\n",
        "    \n",
        "    # print(f'Final evaluation: {timestep_n} timestep')\n",
        "    # print(f'    reward {mean_eval_reward}±{std_eval_reward}')\n",
        "    # print(f'    episode length {mean_eval_length}±{std_eval_length}')\n",
        "    # print(f'    episodes: {episode}')\n",
        "    # print(f'    n_updates: {driver.n_updates}')\n",
        "    # print(f'    epsilon: {driver.epsilon}')\n",
        "\n",
        "    state, info = env.reset()\n",
        "    \n",
        "    episode_reward_list.append(episode_reward)\n",
        "    episode_length_list.append(episode_length)\n",
        "    episode_loss_list.append(np.mean(loss_list))\n",
        "    now_time = datetime.datetime.now()\n",
        "    episode_date_list.append(now_time.date().strftime('%Y-%m-%d'))\n",
        "    episode_time_list.append(now_time.time().strftime('%H:%M:%S'))\n",
        "    draw_check = plot_reward(episode, episode_reward_list, timestep_n)\n",
        "\n",
        "    if episode % when2log == 0:\n",
        "        driver.write_log(\n",
        "            episode_date_list,\n",
        "            episode_time_list,\n",
        "            episode_reward_list,\n",
        "            episode_length_list,\n",
        "            episode_loss_list,\n",
        "            episode_epsilon_list,\n",
        "            log_filename='DQN_log4.csv'\n",
        "        )\n",
        "\n",
        "    # env.close()\n",
        "\n",
        "plt.ioff()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loaded model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make(\"CarRacing-v2\", continuous=False)\n",
        "env = SkipFrame(env, skip=4)\n",
        "env = gym_wrap.GrayScaleObservation(env)\n",
        "env = gym_wrap.ResizeObservation(env, shape=84)\n",
        "env = gym_wrap.FrameStack(env, num_stack=4)\n",
        "state, info = env.reset()\n",
        "action_n = env.action_space.n\n",
        "# driver = Agent(state.shape, action_n,\n",
        "#                load_state='eval', load_model='DQN_740188.pt')\n",
        "driver = Agent(state.shape, action_n,\n",
        "               load_state='eval', load_model='DDQN_743550.pt')\n",
        "driver.epsilon = 0\n",
        "episodes = 10\n",
        "scores_array = []\n",
        "timestep_arr = []\n",
        "seeds_list = [i for i in range(50)]\n",
        "\n",
        "for episode, sd in enumerate(seeds_list):\n",
        "    state, info = env.reset(seed=sd)\n",
        "    updating = True\n",
        "    score = 0\n",
        "    timestep = 0\n",
        "    \n",
        "    while updating:\n",
        "        action = driver.take_action(state) \n",
        "        state, reward, terminated, truncated, info = env.step(action)\n",
        "        updating = not (terminated or truncated)\n",
        "        score += reward\n",
        "        timestep += 1\n",
        "    scores_array.append(score)\n",
        "    timestep_arr.append(timestep)\n",
        "    print(f\"Episode:{episode}, Score:{score:.2f}, Timesteps: {timestep}\")\n",
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
